{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM for non-intrusive reduced order model of a SEIR model in an idealised town"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-35ad2217b7f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meofs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meofs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras.layers\n",
    "import numpy as np\n",
    "import eofs\n",
    "from eofs.standard import Eof\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow.keras as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import save_model\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "plt.rc('axes', labelsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions required to: scale and unscaled data, and to obtain lag-n data to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scalerThetis(x, xmin, xmax, min, max):\n",
    "    scale = (max - min)/(xmax - xmin)\n",
    "    xScaled = scale*x + min - xmin*scale\n",
    "    return xScaled\n",
    "\n",
    "def inverseScalerThetis(xscaled, xmin, xmax, min, max):\n",
    "    scale = (max - min) / (xmax - xmin)\n",
    "    xInv = (xscaled/scale) - (min/scale) + xmin\n",
    "    return xInv\n",
    "\n",
    "def lookBack(X, look_back = 1):\n",
    "    #look_back = 10\n",
    "    X_lb = np.empty((X.shape[0] - look_back + 1, look_back, X.shape[1]))\n",
    "    #X_test = np.empty((look_back, X_test.shape[1], X_train.shape[0] - look_back + 1))\n",
    "    ini = 0\n",
    "    fin = look_back\n",
    "    for i in range(X.shape[0] - look_back + 1):\n",
    "        X_lb[i,:,:] = (X[ini:fin,:])\n",
    "        ini = ini + 1\n",
    "        fin = fin + 1\n",
    "    return X_lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "directory = '/data/'\n",
    "filename = 'group-output-time.csv'\n",
    "\n",
    "import numpy as np\n",
    "csv = np.genfromtxt(directory + filename)\n",
    "\n",
    "ntime = 3888\n",
    "ngroups = 8\n",
    "ngridx = 10\n",
    "ngridy = 10\n",
    "groups_names = ['HOME - S', 'HOME- E', 'HOME - I', 'HOME - R', 'MOBILE - S', 'MOBILE - E', 'MOBILE - I', 'MOBILE - R']\n",
    "\n",
    "csv1 = np.reshape(csv, (ntime, ngroups, ngridx, ngridy))\n",
    "csv2 = np.reshape(csv1, (ntime, ngroups*ngridx*ngridy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask out areas where\n",
    "mask = csv2[500, :].copy()\n",
    "mask = [np.where(mask == 0, 0, 1)]\n",
    "mask = np.array(mask)\n",
    "modeldata = np.zeros((csv2.shape[0], np.count_nonzero(mask)))\n",
    "for i in range(csv2.shape[0]):\n",
    "    test = csv2[i, :]\n",
    "    modeldata[i, :] = test[np.where(mask == 1)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalising per variable\n",
    "sS = modeldata[:, 20*0:20*1]\n",
    "sE = modeldata[:, 20*1:20*2]\n",
    "sI = modeldata[:, 20*2:20*3]\n",
    "sR = modeldata[:, 20*3:20*4]\n",
    "\n",
    "mS = modeldata[:, 20*4:20*5]\n",
    "mE = modeldata[:, 20*5:20*6]\n",
    "mI = modeldata[:, 20*6:20*7]\n",
    "mR = modeldata[:, 20*7:20*8]\n",
    "\n",
    "sSmean = np.mean(modeldata[:, 20*0:20*1], axis = 0)\n",
    "sEmean = np.mean(modeldata[:, 20*1:20*2], axis = 0)\n",
    "sImean = np.mean(modeldata[:, 20*2:20*3], axis = 0)\n",
    "sRmean = np.mean(modeldata[:, 20*3:20*4], axis = 0)\n",
    "\n",
    "mSmean = np.mean(modeldata[:, 20*4:20*5], axis = 0)\n",
    "mEmean = np.mean(modeldata[:, 20*5:20*6], axis = 0)\n",
    "mImean = np.mean(modeldata[:, 20*6:20*7], axis = 0)\n",
    "mRmean = np.mean(modeldata[:, 20*7:20*8], axis = 0)\n",
    "\n",
    "sSsigma = np.std(modeldata[:, 20*0:20*1])\n",
    "sEsigma = np.std(modeldata[:, 20*1:20*2])\n",
    "sIsigma = np.std(modeldata[:, 20*2:20*3])\n",
    "sRsigma = np.std(modeldata[:, 20*3:20*4])\n",
    "\n",
    "mSsigma = np.std(modeldata[:, 20*4:20*5])\n",
    "mEsigma = np.std(modeldata[:, 20*5:20*6])\n",
    "mIsigma = np.std(modeldata[:, 20*6:20*7])\n",
    "mRsigma = np.std(modeldata[:, 20*7:20*8])\n",
    "\n",
    "sSn = (sS - sSmean)/sSsigma\n",
    "sEn = (sE - sEmean)/sEsigma\n",
    "sIn = (sI - sImean)/sIsigma\n",
    "sRn = (sR - sRmean)/sRsigma\n",
    "\n",
    "mSn = (mS - mSmean)/mSsigma\n",
    "mEn = (mE - mEmean)/mEsigma\n",
    "mIn = (mI - mImean)/mIsigma\n",
    "mRn = (mR - mRmean)/mRsigma\n",
    "\n",
    "#Concatenate normalised data\n",
    "\n",
    "modeldataNorm = np.hstack([sSn, sEn, sIn, sRn, mSn, mEn, mIn, mRn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeldataNorm = (modeldata - meandata)/sigmadata\n",
    "targetVariance = 0.999\n",
    "solver = Eof(modeldataNorm)\n",
    "varianceCumulative = np.cumsum(solver.varianceFraction())\n",
    "pcs = solver.pcs()\n",
    "eofs = solver.eofs()\n",
    "trun = 15\n",
    "pcs_trun = pcs[:, :trun]\n",
    "eofs_trun = eofs[:trun, :]\n",
    "\n",
    "x = np.arange(1,16)\n",
    "y1 = solver.eigenvalues()[:15]\n",
    "y2 = varianceCumulative[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=[20,10])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.bar(x, y1)\n",
    "ax1.semilogy()\n",
    "ax1.bar(x, y1)\n",
    "\n",
    "ax1.semilogy()\n",
    "ax2.plot(x, y2, 'g-o', linewidth=5, markersize=10)\n",
    "plt.xlim(0,16)\n",
    "plt.xticks(np.arange(1,16))\n",
    "\n",
    "ax1.set_xlabel('X data')\n",
    "\n",
    "ax1.set_ylabel('Eigenvalues', color = 'orange')\n",
    "ax2.set_ylabel('Cumulative variance', color='g')\n",
    "plt.xlabel('Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct per variable\n",
    "trun_data = np.matmul(pcs_trun, eofs_trun)\n",
    "sStrun = trun_data[:, 20*0:20*1]*sSsigma + sSmean\n",
    "sEtrun = trun_data[:, 20*1:20*2]*sEsigma + sEmean\n",
    "sItrun = trun_data[:, 20*2:20*3]*sIsigma + sImean\n",
    "sRtrun = trun_data[:, 20*3:20*4]*sRsigma + sRmean\n",
    "\n",
    "mStrun = trun_data[:, 20*4:20*5]*mSsigma + mSmean\n",
    "mEtrun = trun_data[:, 20*5:20*6]*mEsigma + mEmean\n",
    "mItrun = trun_data[:, 20*6:20*7]*mIsigma + mImean\n",
    "mRtrun = trun_data[:, 20*7:20*8]*mRsigma + mRmean\n",
    "\n",
    "truncated_data = np.hstack([sStrun, sEtrun, sItrun, sRtrun, mStrun, mEtrun, mItrun, mRtrun])\n",
    "trun_dataf = np.matmul(pcs, eofs)\n",
    "sStrunf = trun_dataf[:, 20*0:20*1]*sSsigma + sSmean\n",
    "sEtrunf = trun_dataf[:, 20*1:20*2]*sEsigma + sEmean\n",
    "sItrunf = trun_dataf[:, 20*2:20*3]*sIsigma + sImean\n",
    "sRtrunf = trun_dataf[:, 20*3:20*4]*sRsigma + sRmean\n",
    "\n",
    "mStrunf = trun_dataf[:, 20*4:20*5]*mSsigma + mSmean\n",
    "mEtrunf = trun_dataf[:, 20*5:20*6]*mEsigma + mEmean\n",
    "mItrunf = trun_dataf[:, 20*6:20*7]*mIsigma + mImean\n",
    "mRtrunf = trun_dataf[:, 20*7:20*8]*mRsigma + mRmean\n",
    "\n",
    "recon_data = np.hstack([sStrunf, sEtrunf, sItrunf, sRtrunf, mStrunf, mEtrunf, mItrunf, mRtrunf])\n",
    "\n",
    "truncated_masked = np.zeros((csv2.shape[0], csv2.shape[1]))\n",
    "recon_masked = np.zeros((csv2.shape[0], csv2.shape[1]))\n",
    "\n",
    "for i in range(csv2.shape[0]):\n",
    "    test = np.zeros(csv2.shape[1])\n",
    "    test[np.where(mask == 1)[1]] = truncated_data[i, :]\n",
    "    truncated_masked[i, :] = test\n",
    "    test = np.zeros(csv2.shape[1])\n",
    "    test[np.where(mask == 1)[1]] = recon_data[i, :]\n",
    "    recon_masked[i, :] = test\n",
    "trun_shaped = np.reshape(truncated_masked,(ntime, ngroups, ngridx, ngridy))\n",
    "recon_shaped = np.reshape(recon_masked,(ntime, ngroups, ngridx, ngridy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed point over time\n",
    "fig = plt.figure(2)\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.plot(csv1[:, i, 0, 5])\n",
    "    plt.plot(trun_shaped[:, i, 0, 4])\n",
    "    plt.plot(recon_shaped[:, i, 0, 4])\n",
    "    plt.title(groups_names[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCS every m time-steps\n",
    "initial_time_step = 0\n",
    "stepBetween = 10\n",
    "pcs_stag = pcs_trun[initial_time_step:, :]\n",
    "pcs_stag = pcs_stag[0:pcs_stag.shape[0]:stepBetween, :]\n",
    "n_stag_time = pcs_stag.shape[0]\n",
    "\n",
    "#Staggered original dataset\n",
    "csv_stag = csv1[initial_time_step:, :, :, :]\n",
    "csv_stag = csv_stag[0:csv_stag.shape[0]:stepBetween, :, :, :]\n",
    "\n",
    "#LSTM model\n",
    "min_ls  = np.min(pcs_stag, 0)\n",
    "max_ls = np.max(pcs_stag, 0)\n",
    "min = 0\n",
    "max = +1\n",
    "\n",
    "look_backX = 8\n",
    "look_backY = 1\n",
    "ls_scaled = scalerThetis(pcs_stag, min_ls, max_ls, min, max)\n",
    "\n",
    "lsX = np.squeeze(ls_scaled[:-look_backY, :])\n",
    "lsy = np.squeeze(ls_scaled[look_backY:, :])\n",
    "X_train, X_test, y_train, y_test = train_test_split(lsX, lsy, test_size=0.1, shuffle=False, random_state=42)\n",
    "\n",
    "y_train = y_train[look_backX - 1:]\n",
    "y_test = y_test[look_backX - 1:]\n",
    "\n",
    "y_train = lookBack(y_train, look_backY)\n",
    "y_test = lookBack(y_test, look_backY)\n",
    "\n",
    "X_train = lookBack(X_train, look_backX)\n",
    "X_test = lookBack(X_test, look_backX)\n",
    "X_all = lookBack(lsX, look_backX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and fit the LSTM network\n",
    "np.random.seed(42)\n",
    "\n",
    "#start_time = time()\n",
    "input_lstm = tf.Input((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_1 = tf.layers.Bidirectional(tf.layers.LSTM(64, return_sequences=False))(input_lstm)\n",
    "dropout_1 = tf.layers.Dropout(0.5)(lstm_1)\n",
    "bn_1 = tf.layers.BatchNormalization()(lstm_1)\n",
    "rv_1 = tf.layers.RepeatVector(look_backY)(bn_1)\n",
    "dense_1 = tf.layers.TimeDistributed(tf.layers.Dense(X_train.shape[2], activation='sigmoid'))(rv_1)\n",
    "\n",
    "lstm_model = tf.Model(input_lstm, dense_1)\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(loss='mean_squared_error', metrics=['mae'], optimizer='nadam')\n",
    "history = lstm_model.fit(X_train, y_train, epochs=500, batch_size=8, verbose=2, validation_data = (X_test, y_test), shuffle = True)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if look_backY == 1:\n",
    "    predLSTM = lstm_model.predict(X_all)\n",
    "    predLSTM = np.squeeze(predLSTM)\n",
    "\n",
    "else:\n",
    "    timesRepeat = int(np.ceil((X_all.shape[0] - look_backX)/look_backY) + 1)\n",
    "    predLSTM = np.empty((0,trun))\n",
    "    for i in range(timesRepeat):\n",
    "        temp = np.squeeze(lstm_model.predict(np.expand_dims(X_all[i*look_backY, :, :], 0)))\n",
    "        predLSTM = np.append(predLSTM, temp, axis=0)\n",
    "\n",
    "predLSTM = inverseScalerThetis(predLSTM, min_ls, max_ls, min, max)\n",
    "fig = plt.figure()\n",
    "#Comparison between PCS and predPCS\n",
    "for i in range(pcs_stag.shape[1]):\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.plot(np.arange(look_backX, n_stag_time), pcs_stag[look_backX:, i])\n",
    "    plt.plot(np.arange(look_backX, n_stag_time), predLSTM[:, i])\n",
    "\n",
    "#predLSTM = np.matmul(predLSTM, eofs_trun)*sigmadata + meandata\n",
    "predLSTM_masked = np.zeros((predLSTM.shape[0], csv2.shape[1]))\n",
    "trun_dataL = np.matmul(predLSTM, eofs_trun)\n",
    "sStrunL = trun_dataL[:, 20*0:20*1]*sSsigma + sSmean\n",
    "sEtrunL = trun_dataL[:, 20*1:20*2]*sEsigma + sEmean\n",
    "sItrunL = trun_dataL[:, 20*2:20*3]*sIsigma + sImean\n",
    "sRtrunL = trun_dataL[:, 20*3:20*4]*sRsigma + sRmean\n",
    "\n",
    "mStrunL = trun_dataL[:, 20*4:20*5]*mSsigma + mSmean\n",
    "mEtrunL = trun_dataL[:, 20*5:20*6]*mEsigma + mEmean\n",
    "mItrunL = trun_dataL[:, 20*6:20*7]*mIsigma + mImean\n",
    "mRtrunL = trun_dataL[:, 20*7:20*8]*mRsigma + mRmean\n",
    "\n",
    "predLSTM = np.hstack([sStrunL, sEtrunL, sItrunL, sRtrunL, mStrunL, mEtrunL, mItrunL, mRtrunL])\n",
    "for i in range(predLSTM.shape[0]):\n",
    "    test = np.zeros(csv2.shape[1])\n",
    "    test[np.where(mask == 1)[1]] = predLSTM[i, :]\n",
    "    predLSTM_masked[i, :] = test\n",
    "predLSTM_shaped = np.reshape(predLSTM_masked,(n_stag_time-look_backX, ngroups, ngridx, ngridy))\n",
    "\n",
    "#Fixed point over time\n",
    "fig = plt.figure(figsize = (20,8))\n",
    "for i in range(8):\n",
    "    plt.subplot(4,2,i+1)\n",
    "    plt.plot(np.arange(look_backX, n_stag_time), csv_stag[look_backX:, i, 0, 4])\n",
    "    plt.plot(np.arange(look_backX, n_stag_time), predLSTM_shaped[:, i, 0, 4])\n",
    "    plt.axvline(x=n_stag_time*0.9, color='g')\n",
    "    plt.title(groups_names[i])\n",
    "    plt.legend(['Ground truth', 'Prediction'])\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
